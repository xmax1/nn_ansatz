{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "statutory-address",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "appointed-affair",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xmax/anaconda3/envs/xmax/lib/python3.8/site-packages/pyscf/lib/misc.py:46: H5pyDeprecationWarning: Using default_file_mode other than 'r' is deprecated. Pass the mode to h5py.File() instead.\n",
      "  h5py.get_config().default_file_mode = 'a'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import jax.numpy as jnp\n",
    "# import jax\n",
    "# import jax.random as rnd\n",
    "\n",
    "from pytorch.models.og.model import fermiNet\n",
    "from pytorch.sampling import MetropolisHasting\n",
    "from pytorch.vmc import get_energy_and_center, compute_local_energy, compute_potential_energy, batched_cdist_l2\n",
    "from pytorch.pretraining_v2 import Pretrainer\n",
    "\n",
    "from pytorch.systems import Molecule\n",
    "import math\n",
    "\n",
    "import torch as tc\n",
    "# tc.set_default_dtype(tc.float64)\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "duplicate-brooks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda:0 cuda:0\n",
      "System: \n",
      " Device  = cuda \n",
      " dtype   = torch.float32 \n",
      " n_atoms = 1 \n",
      " n_up    = 2 \n",
      " n_down  = 2 \n",
      "\n",
      "converged SCF energy = -14.351880476202\n",
      "[[4, (0.0, 0.0, 0.0)]] cuda:0\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/42212810/tqdm-in-jupyter-notebook-prints-new-progress-bars-repeatedly\n",
    "n_iterations = 10000\n",
    "n_samples = 1024\n",
    "n_electrons = 4\n",
    "n_up = 2\n",
    "n_atoms = 1\n",
    "device = 'cuda'\n",
    "r_atoms = tc.zeros((1, 1, 3)).to(device)\n",
    "z_atoms = tc.ones((1,)).to(device) * n_electrons\n",
    "walkers = tc.normal(0., 1., (n_samples, n_electrons, 3)).to(device)\n",
    "print('device: ', walkers.device, r_atoms.device)\n",
    "mol = Molecule(r_atoms, z_atoms, n_electrons, n_up, device=device)\n",
    "print(mol.atom, mol.r_atoms.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "signed-category",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \n",
      " device   = cuda \n",
      " n_sh     = 64 \n",
      " n_ph     = 16 \n",
      " n_layers = 2 \n",
      " n_det    = 8 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "wf = fermiNet(mol)\n",
    "logdet = tc.max(walkers.sum(1), dim=1, keepdims=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "floppy-converter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 1, 4]) torch.Size([1024, 1, 4]) torch.Size([1024, 4, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "x1 = walkers\n",
    "x2 = r_atoms\n",
    "x1_sq = (x1 ** 2).sum(-1, keepdims=True)\n",
    "x2_sq = (x2 ** 2).sum(-1, keepdims=True)\n",
    "cdist = (x1_sq.transpose(-1, -2) + x2_sq - (2 * x1.unsqueeze(1) * x2.unsqueeze(2)).sum(-1)).sqrt()\n",
    "\n",
    "print(cdist.shape, (x1_sq.transpose(-1, -2) + x2_sq).shape, (2 * x1.unsqueeze(2) * x2.unsqueeze(1)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "central-woman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(r_atoms.device)\n",
    "e_locs = compute_local_energy(wf, walkers, r_atoms,  z_atoms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "received-breach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: \n",
      " Device  = cuda \n",
      " dtype   = torch.float32 \n",
      " n_atoms = 1 \n",
      " n_up    = 2 \n",
      " n_down  = 2 \n",
      "\n",
      "converged SCF energy = -14.351880476202\n",
      "Model: \n",
      " device   = cuda \n",
      " n_sh     = 64 \n",
      " n_ph     = 16 \n",
      " n_layers = 2 \n",
      " n_det    = 1 \n",
      "\n",
      "initialized pretraining sampler\n",
      "initialized sampler\n",
      "tensor(-6.3479, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-6.4557, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-6.5678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-6.4556, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-6.5406, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-6.6294, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-6.7044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-6.7864, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-6.6874, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-6.8344, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-6.8104, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-6.8546, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-6.7699, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-6.9025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-6.7279, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-6.8097, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-6.8384, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-6.7870, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-6.9454, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-6.9780, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-6.9288, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-6.9621, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-6.9070, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-6.8305, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-6.8693, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1448, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1326, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.0444, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1934, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1294, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.0405, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2476, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.0908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2361, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1571, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1663, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1507, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1194, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3536, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1721, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2204, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5112, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4136, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4332, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4642, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3594, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4268, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1782, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2411, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3383, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4207, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3864, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4345, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.6627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4075, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4985, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.6018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5185, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4402, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.8133, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.6077, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.8147, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.7868, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.7811, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.6426, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.7783, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5572, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4529, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2127, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3225, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3690, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4222, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5382, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4646, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2089, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2943, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2452, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2316, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4774, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4358, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3458, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3464, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2708, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3353, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.7069, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3704, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2807, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2461, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3213, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2603, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3588, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3228, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2147, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3033, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2600, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2872, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4386, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3298, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2951, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2439, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2585, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1565, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1839, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1064, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1343, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3775, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4978, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3812, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4440, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3301, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3328, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4622, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4194, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4277, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4994, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.6156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4345, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5838, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.6196, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3724, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4661, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2629, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-7.3521, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.6014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4878, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4240, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4185, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4686, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4958, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4345, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.6880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4584, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.8496, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.6250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5486, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.7212, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5907, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4687, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4070, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.6699, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5729, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4765, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3975, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3698, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5683, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-8.1944, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5481, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5576, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5455, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.6066, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.8692, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.6379, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.8280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.7016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.7067, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.6652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5344, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.6831, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3399, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3210, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4363, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3872, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2611, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4485, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3515, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4054, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4802, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5511, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3651, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1368, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2387, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4540, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4669, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5978, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3375, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3265, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3133, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2636, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2654, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3394, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.0970, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4528, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2759, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2632, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4506, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4301, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5325, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.0675, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3856, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3533, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2333, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3138, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3392, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2675, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4208, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3197, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2564, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4607, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1847, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.0883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5363, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5184, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2459, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3797, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3726, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4226, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5440, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4985, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3985, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3719, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3590, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1838, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4411, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4930, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1797, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5574, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4126, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.6082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4055, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2194, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3604, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3919, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5537, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1262, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3855, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3730, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4379, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1505, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3460, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3677, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4324, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3390, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1630, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4536, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3283, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5976, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3410, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2873, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1891, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2990, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3401, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4281, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-7.3780, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4680, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3751, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4159, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4381, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5722, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4591, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.8185, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.6579, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.6350, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3734, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3839, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2249, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5135, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4091, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4383, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4378, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3360, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2872, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4615, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3427, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1753, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2364, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3377, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4721, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2835, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3150, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4179, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3314, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4350, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3295, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4291, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4983, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2603, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3078, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3606, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3066, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5083, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3470, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2333, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2066, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1334, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.0797, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2818, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1816, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3032, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2108, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3812, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3154, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3931, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2554, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3776, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3784, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.6158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.6755, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4439, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5810, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4064, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3400, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.6485, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4388, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3967, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1153, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5727, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3119, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.6115, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3885, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3103, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4341, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5088, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4397, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3588, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5277, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4201, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3460, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.6031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3537, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1556, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3120, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.6083, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3996, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3240, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3515, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1552, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2099, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2886, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3337, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3698, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3811, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4891, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3207, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5395, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4186, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2209, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4757, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3231, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5153, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2520, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.6090, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3258, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3411, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2259, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4542, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4217, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4591, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3559, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4295, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4513, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2589, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2779, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5348, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5995, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3492, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4613, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5226, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3659, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4530, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2543, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2891, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5817, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.6152, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-7.4876, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4945, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4734, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5714, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3795, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4784, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3818, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.6223, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5570, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4520, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1945, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3363, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.8250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1928, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2953, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1901, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4149, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5769, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3834, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5175, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1729, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3371, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.7562, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.7178, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2891, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.7425, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5088, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4989, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5830, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5058, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5272, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5144, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4691, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1780, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3431, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3110, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2693, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3984, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4947, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5135, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5126, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4370, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5393, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4461, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4144, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3839, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2784, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4485, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.6149, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2691, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2324, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4133, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3911, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.4558, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3568, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5597, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3940, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3282, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2599, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2290, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3296, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.1063, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2486, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.2142, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3292, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.5446, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(-7.3896, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95286be77a8f49bc91fc5b4d38ea4b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pretraining:   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 | e_mean -7.5310  | loss 0.32 \n",
      "step 1 | e_mean -7.2638  | loss 0.33 \n",
      "step 2 | e_mean -7.3577  | loss 0.35 \n",
      "step 3 | e_mean -7.4852  | loss 0.35 \n",
      "step 4 | e_mean -7.6206  | loss 0.36 \n",
      "step 5 | e_mean -7.3778  | loss 0.36 \n",
      "step 6 | e_mean -7.7514  | loss 0.34 \n",
      "step 7 | e_mean -7.4669  | loss 0.35 \n",
      "step 8 | e_mean -7.6512  | loss 0.35 \n",
      "step 9 | e_mean -7.6565  | loss 0.34 \n",
      "step 10 | e_mean -7.4818  | loss 0.35 \n",
      "step 11 | e_mean -7.6096  | loss 0.39 \n",
      "step 12 | e_mean -7.7876  | loss 0.39 \n",
      "step 13 | e_mean -7.6861  | loss 0.41 \n",
      "step 14 | e_mean -7.4740  | loss 0.42 \n",
      "step 15 | e_mean -7.3849  | loss 0.41 \n",
      "step 16 | e_mean -7.2693  | loss 0.42 \n",
      "step 17 | e_mean -7.4650  | loss 0.43 \n",
      "step 18 | e_mean -7.5322  | loss 0.43 \n",
      "step 19 | e_mean -7.6332  | loss 0.42 \n",
      "step 20 | e_mean -7.5368  | loss 0.46 \n",
      "step 21 | e_mean -7.3021  | loss 0.45 \n",
      "step 22 | e_mean -7.3789  | loss 0.45 \n",
      "step 23 | e_mean -7.6273  | loss 0.46 \n",
      "step 24 | e_mean -7.2323  | loss 0.46 \n",
      "step 25 | e_mean -7.2319  | loss 0.48 \n",
      "step 26 | e_mean -6.9765  | loss 0.46 \n",
      "step 27 | e_mean -6.9355  | loss 0.49 \n",
      "step 28 | e_mean -6.5837  | loss 0.50 \n",
      "step 29 | e_mean -6.7907  | loss 0.50 \n",
      "step 30 | e_mean -6.6547  | loss 0.50 \n",
      "step 31 | e_mean -6.6390  | loss 0.47 \n",
      "step 32 | e_mean -6.6305  | loss 0.48 \n",
      "step 33 | e_mean -6.6384  | loss 0.49 \n",
      "step 34 | e_mean -6.3662  | loss 0.52 \n",
      "step 35 | e_mean -6.3560  | loss 0.51 \n",
      "step 36 | e_mean -6.2598  | loss 0.54 \n",
      "step 37 | e_mean -6.3167  | loss 0.56 \n",
      "step 38 | e_mean -6.2835  | loss 0.56 \n",
      "step 39 | e_mean -6.4238  | loss 0.53 \n",
      "step 40 | e_mean -6.1836  | loss 0.54 \n",
      "step 41 | e_mean -6.1924  | loss 0.52 \n",
      "step 42 | e_mean -6.3359  | loss 0.52 \n",
      "step 43 | e_mean -6.2540  | loss 0.53 \n",
      "step 44 | e_mean -6.3203  | loss 0.56 \n",
      "step 45 | e_mean -6.1969  | loss 0.55 \n",
      "step 46 | e_mean -6.2747  | loss 0.54 \n",
      "step 47 | e_mean -6.3529  | loss 0.53 \n",
      "step 48 | e_mean -6.1764  | loss 0.50 \n",
      "step 49 | e_mean -6.1561  | loss 0.50 \n",
      "step 50 | e_mean -6.1569  | loss 0.52 \n",
      "step 51 | e_mean -6.1233  | loss 0.51 \n",
      "step 52 | e_mean -5.9822  | loss 0.49 \n",
      "step 53 | e_mean -6.0701  | loss 0.48 \n",
      "step 54 | e_mean -6.1302  | loss 0.48 \n",
      "step 55 | e_mean -5.9950  | loss 0.47 \n",
      "step 56 | e_mean -5.8510  | loss 0.46 \n",
      "step 57 | e_mean -6.0053  | loss 0.46 \n",
      "step 58 | e_mean -5.9879  | loss 0.44 \n",
      "step 59 | e_mean -5.8017  | loss 0.45 \n",
      "step 60 | e_mean -5.7849  | loss 0.45 \n",
      "step 61 | e_mean -5.8357  | loss 0.44 \n",
      "step 62 | e_mean -5.8827  | loss 0.43 \n",
      "step 63 | e_mean -5.8426  | loss 0.44 \n",
      "step 64 | e_mean -5.8780  | loss 0.45 \n",
      "step 65 | e_mean -5.6459  | loss 0.45 \n",
      "step 66 | e_mean -5.8671  | loss 0.46 \n",
      "step 67 | e_mean -5.6966  | loss 0.46 \n",
      "step 68 | e_mean -5.8771  | loss 0.46 \n",
      "step 69 | e_mean -5.7591  | loss 0.47 \n",
      "step 70 | e_mean -6.0802  | loss 0.50 \n",
      "step 71 | e_mean -5.8267  | loss 0.47 \n",
      "step 72 | e_mean -5.6810  | loss 0.47 \n",
      "step 73 | e_mean -5.5096  | loss 0.46 \n",
      "step 74 | e_mean -5.7052  | loss 0.46 \n",
      "step 75 | e_mean -5.6939  | loss 0.47 \n",
      "step 76 | e_mean -5.5581  | loss 0.48 \n",
      "step 77 | e_mean -5.5606  | loss 0.53 \n",
      "step 78 | e_mean -5.5016  | loss 0.52 \n",
      "step 79 | e_mean -5.4705  | loss 0.50 \n",
      "step 80 | e_mean -5.5456  | loss 0.50 \n",
      "step 81 | e_mean -5.5468  | loss 0.50 \n",
      "step 82 | e_mean -5.5518  | loss 0.50 \n",
      "step 83 | e_mean -5.6786  | loss 0.48 \n",
      "step 84 | e_mean -5.6268  | loss 0.47 \n",
      "step 85 | e_mean -5.8334  | loss 0.47 \n",
      "step 86 | e_mean -5.9164  | loss 0.47 \n",
      "step 87 | e_mean -5.8246  | loss 0.44 \n",
      "step 88 | e_mean -5.8789  | loss 0.44 \n",
      "step 89 | e_mean -5.7051  | loss 0.43 \n",
      "step 90 | e_mean -5.6263  | loss 0.44 \n",
      "step 91 | e_mean -5.9042  | loss 0.44 \n",
      "step 92 | e_mean -5.8466  | loss 0.47 \n",
      "step 93 | e_mean -5.8582  | loss 0.46 \n",
      "step 94 | e_mean -5.8859  | loss 0.44 \n",
      "step 95 | e_mean -5.7701  | loss 0.46 \n",
      "step 96 | e_mean -5.8456  | loss 0.47 \n",
      "step 97 | e_mean -6.0289  | loss 0.48 \n",
      "step 98 | e_mean -6.1266  | loss 0.49 \n",
      "step 99 | e_mean -6.0452  | loss 0.50 \n",
      "step 100 | e_mean -6.1001  | loss 0.49 \n",
      "step 101 | e_mean -6.1279  | loss 0.49 \n",
      "step 102 | e_mean -6.0954  | loss 0.49 \n",
      "step 103 | e_mean -6.2076  | loss 0.50 \n",
      "step 104 | e_mean -6.0268  | loss 0.51 \n",
      "step 105 | e_mean -6.2940  | loss 0.50 \n",
      "step 106 | e_mean -6.2022  | loss 0.52 \n",
      "step 107 | e_mean -6.2940  | loss 0.53 \n",
      "step 108 | e_mean -6.2646  | loss 0.52 \n",
      "step 109 | e_mean -6.2172  | loss 0.50 \n",
      "step 110 | e_mean -6.4368  | loss 0.52 \n",
      "step 111 | e_mean -6.3284  | loss 0.51 \n",
      "step 112 | e_mean -6.4807  | loss 0.53 \n",
      "step 113 | e_mean -6.8484  | loss 0.53 \n",
      "step 114 | e_mean -6.5474  | loss 0.51 \n",
      "step 115 | e_mean -6.6469  | loss 0.52 \n",
      "step 116 | e_mean -6.5832  | loss 0.50 \n",
      "step 117 | e_mean -6.5938  | loss 0.51 \n",
      "step 118 | e_mean -6.6813  | loss 0.54 \n",
      "step 119 | e_mean -6.5371  | loss 0.52 \n",
      "step 120 | e_mean -6.7642  | loss 0.52 \n",
      "step 121 | e_mean -6.5518  | loss 0.52 \n",
      "step 122 | e_mean -6.6515  | loss 0.51 \n",
      "step 123 | e_mean -6.4580  | loss 0.49 \n",
      "step 124 | e_mean -6.3725  | loss 0.46 \n",
      "step 125 | e_mean -6.3908  | loss 0.46 \n",
      "step 126 | e_mean -6.4355  | loss 0.47 \n",
      "step 127 | e_mean -6.5709  | loss 0.46 \n",
      "step 128 | e_mean -6.7381  | loss 0.48 \n",
      "step 129 | e_mean -6.5828  | loss 0.46 \n",
      "step 130 | e_mean -6.6183  | loss 0.48 \n",
      "step 131 | e_mean -6.7049  | loss 0.50 \n",
      "step 132 | e_mean -6.5891  | loss 0.49 \n",
      "step 133 | e_mean -6.6511  | loss 0.50 \n",
      "step 134 | e_mean -6.6067  | loss 0.51 \n",
      "step 135 | e_mean -6.4608  | loss 0.54 \n",
      "step 136 | e_mean -6.4915  | loss 0.53 \n",
      "step 137 | e_mean -6.7261  | loss 0.52 \n",
      "step 138 | e_mean -6.6373  | loss 0.53 \n",
      "step 139 | e_mean -6.8533  | loss 0.54 \n",
      "step 140 | e_mean -6.7350  | loss 0.54 \n",
      "step 141 | e_mean -6.9342  | loss 0.51 \n",
      "step 142 | e_mean -6.7634  | loss 0.50 \n",
      "step 143 | e_mean -6.8052  | loss 0.48 \n",
      "step 144 | e_mean -6.8601  | loss 0.48 \n",
      "step 145 | e_mean -6.8404  | loss 0.47 \n",
      "step 146 | e_mean -6.9701  | loss 0.46 \n",
      "step 147 | e_mean -6.7917  | loss 0.44 \n",
      "step 148 | e_mean -6.9419  | loss 0.44 \n",
      "step 149 | e_mean -7.0362  | loss 0.43 \n",
      "step 150 | e_mean -7.0908  | loss 0.44 \n",
      "step 151 | e_mean -7.0939  | loss 0.46 \n",
      "step 152 | e_mean -7.0582  | loss 0.46 \n",
      "step 153 | e_mean -6.9628  | loss 0.46 \n",
      "step 154 | e_mean -7.0327  | loss 0.44 \n",
      "step 155 | e_mean -7.1317  | loss 0.47 \n",
      "step 156 | e_mean -7.1607  | loss 0.44 \n",
      "step 157 | e_mean -7.1219  | loss 0.46 \n",
      "step 158 | e_mean -7.0816  | loss 0.43 \n",
      "step 159 | e_mean -7.0785  | loss 0.45 \n",
      "step 160 | e_mean -7.0539  | loss 0.45 \n",
      "step 161 | e_mean -7.1704  | loss 0.43 \n",
      "step 162 | e_mean -7.0088  | loss 0.43 \n",
      "step 163 | e_mean -7.3002  | loss 0.41 \n",
      "step 164 | e_mean -7.2680  | loss 0.42 \n",
      "step 165 | e_mean -7.3080  | loss 0.42 \n",
      "step 166 | e_mean -7.2950  | loss 0.44 \n",
      "step 167 | e_mean -7.4249  | loss 0.45 \n",
      "step 168 | e_mean -7.3487  | loss 0.44 \n",
      "step 169 | e_mean -7.2561  | loss 0.44 \n",
      "step 170 | e_mean -7.5791  | loss 0.45 \n",
      "step 171 | e_mean -7.4517  | loss 0.44 \n",
      "step 172 | e_mean -7.3487  | loss 0.44 \n",
      "step 173 | e_mean -7.2860  | loss 0.46 \n",
      "step 174 | e_mean -7.3011  | loss 0.45 \n",
      "step 175 | e_mean -7.2417  | loss 0.47 \n",
      "step 176 | e_mean -7.5236  | loss 0.46 \n",
      "step 177 | e_mean -7.3923  | loss 0.47 \n",
      "step 178 | e_mean -7.6097  | loss 0.48 \n",
      "step 179 | e_mean -7.5792  | loss 0.49 \n",
      "step 180 | e_mean -7.5035  | loss 0.48 \n",
      "step 181 | e_mean -7.5677  | loss 0.46 \n",
      "step 182 | e_mean -7.4038  | loss 0.48 \n",
      "step 183 | e_mean -7.6744  | loss 0.48 \n",
      "step 184 | e_mean -7.6608  | loss 0.47 \n",
      "step 185 | e_mean -7.8926  | loss 0.49 \n",
      "step 186 | e_mean -7.5931  | loss 0.48 \n",
      "step 187 | e_mean -7.7894  | loss 0.45 \n",
      "step 188 | e_mean -7.4718  | loss 0.45 \n",
      "step 189 | e_mean -7.5996  | loss 0.45 \n",
      "step 190 | e_mean -7.7687  | loss 0.45 \n",
      "step 191 | e_mean -7.4949  | loss 0.46 \n",
      "step 192 | e_mean -7.7716  | loss 0.48 \n",
      "step 193 | e_mean -7.6287  | loss 0.47 \n",
      "step 194 | e_mean -7.8947  | loss 0.47 \n",
      "step 195 | e_mean -7.7257  | loss 0.46 \n",
      "step 196 | e_mean -7.6672  | loss 0.46 \n",
      "step 197 | e_mean -7.7418  | loss 0.46 \n",
      "step 198 | e_mean -7.6588  | loss 0.47 \n",
      "step 199 | e_mean -7.7283  | loss 0.47 \n",
      "step 200 | e_mean -7.5385  | loss 0.49 \n",
      "step 201 | e_mean -7.4476  | loss 0.47 \n",
      "step 202 | e_mean -7.7140  | loss 0.44 \n",
      "step 203 | e_mean -7.4604  | loss 0.44 \n",
      "step 204 | e_mean -7.7087  | loss 0.45 \n",
      "step 205 | e_mean -7.7565  | loss 0.44 \n",
      "step 206 | e_mean -7.7761  | loss 0.43 \n",
      "step 207 | e_mean -7.6227  | loss 0.46 \n",
      "step 208 | e_mean -7.6987  | loss 0.48 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 209 | e_mean -7.6028  | loss 0.48 \n",
      "step 210 | e_mean -7.5088  | loss 0.48 \n",
      "step 211 | e_mean -7.7122  | loss 0.48 \n",
      "step 212 | e_mean -7.8593  | loss 0.49 \n",
      "step 213 | e_mean -7.6914  | loss 0.48 \n",
      "step 214 | e_mean -7.7024  | loss 0.47 \n",
      "step 215 | e_mean -7.6920  | loss 0.48 \n",
      "step 216 | e_mean -7.7132  | loss 0.49 \n",
      "step 217 | e_mean -7.8542  | loss 0.49 \n",
      "step 218 | e_mean -7.9383  | loss 0.50 \n",
      "step 219 | e_mean -8.0562  | loss 0.49 \n",
      "step 220 | e_mean -7.8327  | loss 0.48 \n",
      "step 221 | e_mean -7.7288  | loss 0.48 \n",
      "step 222 | e_mean -7.6135  | loss 0.46 \n",
      "step 223 | e_mean -7.5969  | loss 0.47 \n",
      "step 224 | e_mean -7.7389  | loss 0.48 \n",
      "step 225 | e_mean -7.8154  | loss 0.49 \n",
      "step 226 | e_mean -7.7435  | loss 0.48 \n",
      "step 227 | e_mean -7.5461  | loss 0.51 \n",
      "step 228 | e_mean -7.7596  | loss 0.50 \n",
      "step 229 | e_mean -7.5927  | loss 0.51 \n",
      "step 230 | e_mean -8.0344  | loss 0.54 \n",
      "step 231 | e_mean -7.9186  | loss 0.53 \n",
      "step 232 | e_mean -7.9092  | loss 0.56 \n",
      "step 233 | e_mean -7.8702  | loss 0.57 \n",
      "step 234 | e_mean -7.9716  | loss 0.53 \n",
      "step 235 | e_mean -7.9188  | loss 0.53 \n",
      "step 236 | e_mean -7.8025  | loss 0.49 \n",
      "step 237 | e_mean -7.7479  | loss 0.48 \n",
      "step 238 | e_mean -7.9954  | loss 0.47 \n",
      "step 239 | e_mean -8.1287  | loss 0.47 \n",
      "step 240 | e_mean -8.1370  | loss 0.48 \n",
      "step 241 | e_mean -7.9753  | loss 0.49 \n",
      "step 242 | e_mean -8.1789  | loss 0.49 \n",
      "step 243 | e_mean -8.4207  | loss 0.50 \n",
      "step 244 | e_mean -7.9068  | loss 0.49 \n",
      "step 245 | e_mean -8.1393  | loss 0.50 \n",
      "step 246 | e_mean -8.0603  | loss 0.50 \n",
      "step 247 | e_mean -7.8630  | loss 0.51 \n",
      "step 248 | e_mean -8.1236  | loss 0.49 \n",
      "step 249 | e_mean -7.9436  | loss 0.50 \n",
      "step 250 | e_mean -8.0634  | loss 0.50 \n",
      "step 251 | e_mean -8.0556  | loss 0.53 \n",
      "step 252 | e_mean -8.0108  | loss 0.52 \n",
      "step 253 | e_mean -8.0640  | loss 0.49 \n",
      "step 254 | e_mean -8.1991  | loss 0.48 \n",
      "step 255 | e_mean -8.0179  | loss 0.47 \n",
      "step 256 | e_mean -7.9769  | loss 0.46 \n",
      "step 257 | e_mean -7.8260  | loss 0.48 \n",
      "step 258 | e_mean -8.4389  | loss 0.48 \n",
      "step 259 | e_mean -7.9140  | loss 0.49 \n",
      "step 260 | e_mean -8.3037  | loss 0.49 \n",
      "step 261 | e_mean -8.1011  | loss 0.47 \n",
      "step 262 | e_mean -8.1324  | loss 0.44 \n",
      "step 263 | e_mean -8.1128  | loss 0.46 \n",
      "step 264 | e_mean -8.1410  | loss 0.47 \n",
      "step 265 | e_mean -8.1516  | loss 0.48 \n",
      "step 266 | e_mean -8.2483  | loss 0.50 \n",
      "step 267 | e_mean -8.4143  | loss 0.51 \n",
      "step 268 | e_mean -8.3505  | loss 0.52 \n",
      "step 269 | e_mean -8.2442  | loss 0.53 \n",
      "step 270 | e_mean -8.2273  | loss 0.54 \n",
      "step 271 | e_mean -8.2544  | loss 0.53 \n",
      "step 272 | e_mean -8.2974  | loss 0.51 \n",
      "step 273 | e_mean -8.3699  | loss 0.51 \n",
      "step 274 | e_mean -8.2587  | loss 0.51 \n",
      "step 275 | e_mean -8.4574  | loss 0.50 \n",
      "step 276 | e_mean -8.2655  | loss 0.50 \n",
      "step 277 | e_mean -8.1019  | loss 0.49 \n",
      "step 278 | e_mean -8.2623  | loss 0.49 \n",
      "step 279 | e_mean -8.2453  | loss 0.52 \n",
      "step 280 | e_mean -8.3800  | loss 0.52 \n",
      "step 281 | e_mean -8.2185  | loss 0.53 \n",
      "step 282 | e_mean -8.5465  | loss 0.53 \n",
      "step 283 | e_mean -8.2943  | loss 0.52 \n",
      "step 284 | e_mean -8.4978  | loss 0.51 \n",
      "step 285 | e_mean -8.2610  | loss 0.51 \n",
      "step 286 | e_mean -8.3559  | loss 0.52 \n",
      "step 287 | e_mean -8.4773  | loss 0.51 \n",
      "step 288 | e_mean -8.5119  | loss 0.53 \n",
      "step 289 | e_mean -8.5781  | loss 0.57 \n",
      "step 290 | e_mean -8.4723  | loss 0.54 \n",
      "step 291 | e_mean -8.4247  | loss 0.54 \n",
      "step 292 | e_mean -8.2681  | loss 0.54 \n",
      "step 293 | e_mean -8.4367  | loss 0.53 \n",
      "step 294 | e_mean -8.5164  | loss 0.53 \n",
      "step 295 | e_mean -8.3800  | loss 0.54 \n",
      "step 296 | e_mean -8.2967  | loss 0.53 \n",
      "step 297 | e_mean -8.2402  | loss 0.54 \n",
      "step 298 | e_mean -8.3907  | loss 0.52 \n",
      "step 299 | e_mean -8.1927  | loss 0.52 \n",
      "step 300 | e_mean -8.3230  | loss 0.51 \n",
      "step 301 | e_mean -8.3848  | loss 0.51 \n",
      "step 302 | e_mean -8.3313  | loss 0.51 \n",
      "step 303 | e_mean -8.4673  | loss 0.51 \n",
      "step 304 | e_mean -8.1774  | loss 0.50 \n",
      "step 305 | e_mean -8.3787  | loss 0.52 \n",
      "step 306 | e_mean -8.4006  | loss 0.51 \n",
      "step 307 | e_mean -8.4006  | loss 0.51 \n",
      "step 308 | e_mean -8.4434  | loss 0.48 \n",
      "step 309 | e_mean -8.6919  | loss 0.48 \n",
      "step 310 | e_mean -8.4175  | loss 0.47 \n",
      "step 311 | e_mean -8.5903  | loss 0.46 \n",
      "step 312 | e_mean -8.4909  | loss 0.47 \n",
      "step 313 | e_mean -8.7732  | loss 0.46 \n",
      "step 314 | e_mean -8.3929  | loss 0.47 \n",
      "step 315 | e_mean -8.6714  | loss 0.46 \n",
      "step 316 | e_mean -8.3459  | loss 0.46 \n",
      "step 317 | e_mean -8.4138  | loss 0.47 \n",
      "step 318 | e_mean -8.4429  | loss 0.47 \n",
      "step 319 | e_mean -8.5389  | loss 0.47 \n",
      "step 320 | e_mean -8.5931  | loss 0.48 \n",
      "step 321 | e_mean -8.2336  | loss 0.47 \n",
      "step 322 | e_mean -8.5624  | loss 0.46 \n",
      "step 323 | e_mean -8.5533  | loss 0.47 \n",
      "step 324 | e_mean -8.6088  | loss 0.45 \n",
      "step 325 | e_mean -8.5509  | loss 0.43 \n",
      "step 326 | e_mean -8.5247  | loss 0.43 \n",
      "step 327 | e_mean -8.6988  | loss 0.44 \n",
      "step 328 | e_mean -8.8887  | loss 0.44 \n",
      "step 329 | e_mean -8.5813  | loss 0.44 \n",
      "step 330 | e_mean -8.6377  | loss 0.45 \n",
      "step 331 | e_mean -8.8261  | loss 0.45 \n",
      "step 332 | e_mean -8.5948  | loss 0.42 \n",
      "step 333 | e_mean -8.8312  | loss 0.44 \n",
      "step 334 | e_mean -8.5432  | loss 0.45 \n",
      "step 335 | e_mean -8.5006  | loss 0.46 \n",
      "step 336 | e_mean -8.7341  | loss 0.45 \n",
      "step 337 | e_mean -8.5903  | loss 0.43 \n",
      "step 338 | e_mean -8.6297  | loss 0.42 \n",
      "step 339 | e_mean -8.9489  | loss 0.41 \n",
      "step 340 | e_mean -8.7239  | loss 0.43 \n",
      "step 341 | e_mean -8.8237  | loss 0.44 \n",
      "step 342 | e_mean -8.6917  | loss 0.44 \n",
      "step 343 | e_mean -8.5746  | loss 0.43 \n",
      "step 344 | e_mean -8.7360  | loss 0.41 \n",
      "step 345 | e_mean -8.9851  | loss 0.39 \n",
      "step 346 | e_mean -8.7658  | loss 0.39 \n",
      "step 347 | e_mean -8.6528  | loss 0.41 \n",
      "step 348 | e_mean -8.6878  | loss 0.40 \n",
      "step 349 | e_mean -9.0139  | loss 0.40 \n",
      "step 350 | e_mean -8.6970  | loss 0.39 \n",
      "step 351 | e_mean -8.5622  | loss 0.36 \n",
      "step 352 | e_mean -8.8038  | loss 0.40 \n",
      "step 353 | e_mean -8.7181  | loss 0.40 \n",
      "step 354 | e_mean -8.4653  | loss 0.39 \n",
      "step 355 | e_mean -8.5993  | loss 0.41 \n",
      "step 356 | e_mean -8.6476  | loss 0.41 \n",
      "step 357 | e_mean -8.9552  | loss 0.39 \n",
      "step 358 | e_mean -8.5792  | loss 0.40 \n",
      "step 359 | e_mean -8.7285  | loss 0.41 \n",
      "step 360 | e_mean -8.6717  | loss 0.43 \n",
      "step 361 | e_mean -8.6163  | loss 0.41 \n",
      "step 362 | e_mean -9.0677  | loss 0.40 \n",
      "step 363 | e_mean -8.8501  | loss 0.38 \n",
      "step 364 | e_mean -8.6829  | loss 0.41 \n",
      "step 365 | e_mean -8.6686  | loss 0.39 \n",
      "step 366 | e_mean -8.9393  | loss 0.38 \n",
      "step 367 | e_mean -8.4837  | loss 0.37 \n",
      "step 368 | e_mean -8.7711  | loss 0.36 \n",
      "step 369 | e_mean -8.5507  | loss 0.36 \n",
      "step 370 | e_mean -8.5143  | loss 0.36 \n",
      "step 371 | e_mean -8.5994  | loss 0.35 \n",
      "step 372 | e_mean -8.9427  | loss 0.34 \n",
      "step 373 | e_mean -8.7248  | loss 0.36 \n",
      "step 374 | e_mean -8.5633  | loss 0.37 \n",
      "step 375 | e_mean -8.6060  | loss 0.40 \n",
      "step 376 | e_mean -8.8592  | loss 0.40 \n",
      "step 377 | e_mean -8.7814  | loss 0.40 \n",
      "step 378 | e_mean -8.9488  | loss 0.41 \n",
      "step 379 | e_mean -8.9257  | loss 0.38 \n",
      "step 380 | e_mean -8.9068  | loss 0.38 \n",
      "step 381 | e_mean -8.8454  | loss 0.37 \n",
      "step 382 | e_mean -8.9602  | loss 0.38 \n",
      "step 383 | e_mean -8.8325  | loss 0.39 \n",
      "step 384 | e_mean -9.0223  | loss 0.41 \n",
      "step 385 | e_mean -8.9606  | loss 0.39 \n",
      "step 386 | e_mean -9.0926  | loss 0.40 \n",
      "step 387 | e_mean -9.1158  | loss 0.39 \n",
      "step 388 | e_mean -8.8557  | loss 0.39 \n",
      "step 389 | e_mean -8.7938  | loss 0.42 \n",
      "step 390 | e_mean -9.2339  | loss 0.41 \n",
      "step 391 | e_mean -9.2654  | loss 0.42 \n",
      "step 392 | e_mean -9.1032  | loss 0.42 \n",
      "step 393 | e_mean -9.0722  | loss 0.42 \n",
      "step 394 | e_mean -9.1432  | loss 0.42 \n",
      "step 395 | e_mean -9.0828  | loss 0.42 \n",
      "step 396 | e_mean -9.1525  | loss 0.44 \n",
      "step 397 | e_mean -9.2022  | loss 0.43 \n",
      "step 398 | e_mean -8.9826  | loss 0.43 \n",
      "step 399 | e_mean -8.9529  | loss 0.42 \n",
      "step 400 | e_mean -9.1445  | loss 0.41 \n",
      "step 401 | e_mean -9.3289  | loss 0.42 \n",
      "step 402 | e_mean -9.3669  | loss 0.41 \n",
      "step 403 | e_mean -9.1604  | loss 0.41 \n",
      "step 404 | e_mean -9.4465  | loss 0.40 \n",
      "step 405 | e_mean -9.1192  | loss 0.41 \n",
      "step 406 | e_mean -9.2889  | loss 0.40 \n",
      "step 407 | e_mean -9.1851  | loss 0.40 \n",
      "step 408 | e_mean -9.4310  | loss 0.40 \n",
      "step 409 | e_mean -9.4103  | loss 0.41 \n",
      "step 410 | e_mean -9.3376  | loss 0.42 \n",
      "step 411 | e_mean -9.3090  | loss 0.41 \n",
      "step 412 | e_mean -9.0640  | loss 0.42 \n",
      "step 413 | e_mean -9.2827  | loss 0.44 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 414 | e_mean -9.1619  | loss 0.44 \n",
      "step 415 | e_mean -9.1742  | loss 0.42 \n",
      "step 416 | e_mean -9.1901  | loss 0.41 \n",
      "step 417 | e_mean -9.1486  | loss 0.42 \n",
      "step 418 | e_mean -9.3015  | loss 0.40 \n",
      "step 419 | e_mean -9.3536  | loss 0.40 \n",
      "step 420 | e_mean -9.6634  | loss 0.41 \n",
      "step 421 | e_mean -9.3768  | loss 0.40 \n",
      "step 422 | e_mean -9.4758  | loss 0.40 \n",
      "step 423 | e_mean -9.5651  | loss 0.39 \n",
      "step 424 | e_mean -9.2615  | loss 0.38 \n",
      "step 425 | e_mean -9.3409  | loss 0.36 \n",
      "step 426 | e_mean -9.2692  | loss 0.37 \n",
      "step 427 | e_mean -9.4488  | loss 0.36 \n",
      "step 428 | e_mean -9.3096  | loss 0.37 \n",
      "step 429 | e_mean -9.4384  | loss 0.37 \n",
      "step 430 | e_mean -9.5389  | loss 0.36 \n",
      "step 431 | e_mean -9.4862  | loss 0.35 \n",
      "step 432 | e_mean -9.6446  | loss 0.38 \n",
      "step 433 | e_mean -9.4137  | loss 0.37 \n",
      "step 434 | e_mean -9.4967  | loss 0.38 \n",
      "step 435 | e_mean -9.5188  | loss 0.40 \n",
      "step 436 | e_mean -9.4954  | loss 0.39 \n",
      "step 437 | e_mean -9.5102  | loss 0.39 \n",
      "step 438 | e_mean -9.5708  | loss 0.39 \n",
      "step 439 | e_mean -9.6474  | loss 0.38 \n",
      "step 440 | e_mean -9.4578  | loss 0.37 \n",
      "step 441 | e_mean -9.8202  | loss 0.37 \n",
      "step 442 | e_mean -9.5132  | loss 0.35 \n",
      "step 443 | e_mean -9.4792  | loss 0.35 \n",
      "step 444 | e_mean -9.7419  | loss 0.37 \n",
      "step 445 | e_mean -9.4846  | loss 0.39 \n",
      "step 446 | e_mean -9.5271  | loss 0.39 \n",
      "step 447 | e_mean -9.7843  | loss 0.38 \n",
      "step 448 | e_mean -9.6034  | loss 0.39 \n",
      "step 449 | e_mean -9.5128  | loss 0.38 \n",
      "step 450 | e_mean -9.4056  | loss 0.35 \n",
      "step 451 | e_mean -9.8106  | loss 0.39 \n",
      "step 452 | e_mean -9.7090  | loss 0.40 \n",
      "step 453 | e_mean -9.8214  | loss 0.39 \n",
      "step 454 | e_mean -9.7792  | loss 0.38 \n",
      "step 455 | e_mean -10.0033  | loss 0.38 \n",
      "step 456 | e_mean -9.8695  | loss 0.39 \n",
      "step 457 | e_mean -9.7523  | loss 0.39 \n",
      "step 458 | e_mean -9.5455  | loss 0.38 \n",
      "step 459 | e_mean -9.8946  | loss 0.38 \n",
      "step 460 | e_mean -10.0077  | loss 0.37 \n",
      "step 461 | e_mean -10.0353  | loss 0.38 \n",
      "step 462 | e_mean -9.5004  | loss 0.38 \n",
      "step 463 | e_mean -9.6797  | loss 0.37 \n",
      "step 464 | e_mean -9.6782  | loss 0.37 \n",
      "step 465 | e_mean -9.7002  | loss 0.35 \n",
      "step 466 | e_mean -9.8806  | loss 0.36 \n",
      "step 467 | e_mean -9.7640  | loss 0.35 \n",
      "step 468 | e_mean -9.7828  | loss 0.35 \n",
      "step 469 | e_mean -9.9435  | loss 0.36 \n",
      "step 470 | e_mean -9.9530  | loss 0.35 \n",
      "step 471 | e_mean -9.8271  | loss 0.36 \n",
      "step 472 | e_mean -10.0218  | loss 0.35 \n",
      "step 473 | e_mean -10.1727  | loss 0.36 \n",
      "step 474 | e_mean -9.9156  | loss 0.36 \n",
      "step 475 | e_mean -10.2071  | loss 0.36 \n",
      "step 476 | e_mean -9.9789  | loss 0.34 \n",
      "step 477 | e_mean -10.2078  | loss 0.32 \n",
      "step 478 | e_mean -10.1051  | loss 0.30 \n",
      "step 479 | e_mean -9.9629  | loss 0.32 \n",
      "step 480 | e_mean -10.1422  | loss 0.31 \n",
      "step 481 | e_mean -10.1346  | loss 0.30 \n",
      "step 482 | e_mean -10.1946  | loss 0.32 \n",
      "step 483 | e_mean -10.2810  | loss 0.32 \n",
      "step 484 | e_mean -10.3972  | loss 0.29 \n",
      "step 485 | e_mean -10.0520  | loss 0.30 \n",
      "step 486 | e_mean -10.0400  | loss 0.29 \n",
      "step 487 | e_mean -9.9888  | loss 0.29 \n",
      "step 488 | e_mean -10.1440  | loss 0.29 \n",
      "step 489 | e_mean -9.9535  | loss 0.30 \n",
      "step 490 | e_mean -10.3453  | loss 0.29 \n",
      "step 491 | e_mean -10.3902  | loss 0.28 \n",
      "step 492 | e_mean -10.6296  | loss 0.29 \n",
      "step 493 | e_mean -9.8985  | loss 0.30 \n",
      "step 494 | e_mean -10.2825  | loss 0.30 \n",
      "step 495 | e_mean -10.1800  | loss 0.29 \n",
      "step 496 | e_mean -10.4160  | loss 0.28 \n",
      "step 497 | e_mean -10.5551  | loss 0.29 \n",
      "step 498 | e_mean -10.5506  | loss 0.27 \n",
      "step 499 | e_mean -10.3277  | loss 0.27 \n",
      "step 500 | e_mean -10.3199  | loss 0.27 \n",
      "step 501 | e_mean -10.5281  | loss 0.27 \n",
      "step 502 | e_mean -10.0867  | loss 0.27 \n",
      "step 503 | e_mean -10.3809  | loss 0.27 \n",
      "step 504 | e_mean -10.3977  | loss 0.26 \n",
      "step 505 | e_mean -10.3811  | loss 0.26 \n",
      "step 506 | e_mean -10.3434  | loss 0.25 \n",
      "step 507 | e_mean -10.7555  | loss 0.26 \n",
      "step 508 | e_mean -10.2949  | loss 0.27 \n",
      "step 509 | e_mean -10.5690  | loss 0.27 \n",
      "step 510 | e_mean -10.4371  | loss 0.27 \n",
      "step 511 | e_mean -10.6198  | loss 0.28 \n",
      "step 512 | e_mean -10.7304  | loss 0.28 \n",
      "step 513 | e_mean -10.5633  | loss 0.26 \n",
      "step 514 | e_mean -10.6959  | loss 0.27 \n",
      "step 515 | e_mean -10.6507  | loss 0.27 \n",
      "step 516 | e_mean -10.8090  | loss 0.26 \n",
      "step 517 | e_mean -10.7369  | loss 0.28 \n",
      "step 518 | e_mean -10.5895  | loss 0.28 \n",
      "step 519 | e_mean -10.4051  | loss 0.31 \n",
      "step 520 | e_mean -10.7136  | loss 0.29 \n",
      "step 521 | e_mean -10.7582  | loss 0.29 \n",
      "step 522 | e_mean -10.9197  | loss 0.29 \n",
      "step 523 | e_mean -10.6219  | loss 0.29 \n",
      "step 524 | e_mean -10.4214  | loss 0.28 \n",
      "step 525 | e_mean -10.7981  | loss 0.27 \n",
      "step 526 | e_mean -10.7833  | loss 0.26 \n",
      "step 527 | e_mean -10.8255  | loss 0.26 \n",
      "step 528 | e_mean -10.7306  | loss 0.26 \n",
      "step 529 | e_mean -10.4889  | loss 0.24 \n",
      "step 530 | e_mean -10.7365  | loss 0.26 \n",
      "step 531 | e_mean -10.6217  | loss 0.26 \n",
      "step 532 | e_mean -10.5866  | loss 0.25 \n",
      "step 533 | e_mean -10.7153  | loss 0.23 \n",
      "step 534 | e_mean -10.9880  | loss 0.22 \n",
      "step 535 | e_mean -10.8086  | loss 0.22 \n",
      "step 536 | e_mean -10.9092  | loss 0.23 \n",
      "step 537 | e_mean -10.9394  | loss 0.23 \n",
      "step 538 | e_mean -11.0511  | loss 0.23 \n",
      "step 539 | e_mean -10.7382  | loss 0.23 \n",
      "step 540 | e_mean -10.9380  | loss 0.24 \n",
      "step 541 | e_mean -10.8121  | loss 0.24 \n",
      "step 542 | e_mean -10.8889  | loss 0.23 \n",
      "step 543 | e_mean -10.7271  | loss 0.23 \n",
      "step 544 | e_mean -10.7236  | loss 0.22 \n",
      "step 545 | e_mean -10.5954  | loss 0.24 \n",
      "step 546 | e_mean -10.6772  | loss 0.23 \n",
      "step 547 | e_mean -11.0081  | loss 0.23 \n",
      "step 548 | e_mean -11.0339  | loss 0.24 \n",
      "step 549 | e_mean -10.9842  | loss 0.24 \n",
      "step 550 | e_mean -11.1330  | loss 0.25 \n",
      "step 551 | e_mean -11.3230  | loss 0.25 \n",
      "step 552 | e_mean -11.0299  | loss 0.24 \n",
      "step 553 | e_mean -10.7041  | loss 0.24 \n",
      "step 554 | e_mean -10.7809  | loss 0.24 \n",
      "step 555 | e_mean -11.0038  | loss 0.24 \n",
      "step 556 | e_mean -10.8507  | loss 0.24 \n",
      "step 557 | e_mean -10.8929  | loss 0.24 \n",
      "step 558 | e_mean -10.9238  | loss 0.23 \n",
      "step 559 | e_mean -11.0039  | loss 0.23 \n",
      "step 560 | e_mean -10.8118  | loss 0.23 \n",
      "step 561 | e_mean -11.1546  | loss 0.24 \n",
      "step 562 | e_mean -10.8717  | loss 0.24 \n",
      "step 563 | e_mean -10.9217  | loss 0.24 \n",
      "step 564 | e_mean -11.1214  | loss 0.25 \n",
      "step 565 | e_mean -10.9839  | loss 0.26 \n",
      "step 566 | e_mean -11.0390  | loss 0.26 \n",
      "step 567 | e_mean -11.0350  | loss 0.26 \n",
      "step 568 | e_mean -10.9250  | loss 0.26 \n",
      "step 569 | e_mean -11.1588  | loss 0.23 \n",
      "step 570 | e_mean -10.9225  | loss 0.23 \n",
      "step 571 | e_mean -10.9208  | loss 0.24 \n",
      "step 572 | e_mean -11.2833  | loss 0.25 \n",
      "step 573 | e_mean -11.1067  | loss 0.24 \n",
      "step 574 | e_mean -11.0826  | loss 0.23 \n",
      "step 575 | e_mean -11.4398  | loss 0.22 \n",
      "step 576 | e_mean -11.0451  | loss 0.23 \n",
      "step 577 | e_mean -10.8081  | loss 0.23 \n",
      "step 578 | e_mean -11.0112  | loss 0.23 \n",
      "step 579 | e_mean -10.9698  | loss 0.23 \n",
      "step 580 | e_mean -11.2197  | loss 0.24 \n",
      "step 581 | e_mean -11.2856  | loss 0.24 \n",
      "step 582 | e_mean -11.6660  | loss 0.22 \n",
      "step 583 | e_mean -10.9772  | loss 0.22 \n",
      "step 584 | e_mean -11.1396  | loss 0.22 \n",
      "step 585 | e_mean -11.3703  | loss 0.23 \n",
      "step 586 | e_mean -11.0846  | loss 0.23 \n",
      "step 587 | e_mean -11.1008  | loss 0.23 \n",
      "step 588 | e_mean -10.9982  | loss 0.23 \n",
      "step 589 | e_mean -11.0964  | loss 0.23 \n",
      "step 590 | e_mean -11.0404  | loss 0.23 \n",
      "step 591 | e_mean -11.1675  | loss 0.24 \n",
      "step 592 | e_mean -11.2301  | loss 0.23 \n",
      "step 593 | e_mean -11.1999  | loss 0.25 \n",
      "step 594 | e_mean -11.3313  | loss 0.24 \n",
      "step 595 | e_mean -11.1200  | loss 0.23 \n",
      "step 596 | e_mean -11.3525  | loss 0.22 \n",
      "step 597 | e_mean -11.1241  | loss 0.22 \n",
      "step 598 | e_mean -11.1675  | loss 0.22 \n",
      "step 599 | e_mean -11.2967  | loss 0.20 \n",
      "step 600 | e_mean -11.2274  | loss 0.20 \n",
      "step 601 | e_mean -11.4211  | loss 0.21 \n",
      "step 602 | e_mean -11.2206  | loss 0.22 \n",
      "step 603 | e_mean -11.1767  | loss 0.22 \n",
      "step 604 | e_mean -11.4089  | loss 0.21 \n",
      "step 605 | e_mean -11.2406  | loss 0.21 \n",
      "step 606 | e_mean -11.1726  | loss 0.21 \n",
      "step 607 | e_mean -11.2304  | loss 0.21 \n",
      "step 608 | e_mean -11.3226  | loss 0.21 \n",
      "step 609 | e_mean -11.4472  | loss 0.22 \n",
      "step 610 | e_mean -11.3965  | loss 0.21 \n",
      "step 611 | e_mean -11.5567  | loss 0.22 \n",
      "step 612 | e_mean -11.2225  | loss 0.22 \n",
      "step 613 | e_mean -11.4086  | loss 0.22 \n",
      "step 614 | e_mean -11.1676  | loss 0.23 \n",
      "step 615 | e_mean -11.5070  | loss 0.25 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 616 | e_mean -11.4651  | loss 0.26 \n",
      "step 617 | e_mean -11.3626  | loss 0.26 \n",
      "step 618 | e_mean -11.4749  | loss 0.23 \n",
      "step 619 | e_mean -11.6331  | loss 0.23 \n",
      "step 620 | e_mean -11.2859  | loss 0.23 \n",
      "step 621 | e_mean -11.4232  | loss 0.25 \n",
      "step 622 | e_mean -11.1486  | loss 0.25 \n",
      "step 623 | e_mean -11.4117  | loss 0.25 \n",
      "step 624 | e_mean -11.0962  | loss 0.25 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-fc1e6fcdd84f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mpretrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mpretrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwalkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_it\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_ansatz/src/pytorch/pretraining_v2.py\u001b[0m in \u001b[0;36mpretrain\u001b[0;34m(self, wf, walkers, n_it, lr)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mwf_walkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwf_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwf_sampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwf_walkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0me_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_local_energy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwf_walkers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr_atoms\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_atoms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xmax/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_ansatz/src/pytorch/sampling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, curr_samples)\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;31m# next sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mnew_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_samples\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0mnew_amps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0mnew_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_amps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xmax/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_ansatz/src/pytorch/models/og/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, walkers)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwalkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0mup_orbitals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdown_orbitals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_orbitals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwalkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;31m# logabssumdet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_ansatz/src/pytorch/models/og/model.py\u001b[0m in \u001b[0;36mgenerate_orbitals\u001b[0;34m(self, walkers)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0mfactor_down\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_down_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_down\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0mexponent_up\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_up_sigma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mae_vectors_up\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0mexponent_down\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_down_sigma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mae_vectors_down\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xmax/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nn_ansatz/src/pytorch/models/og/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, ae_vectors)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mae_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mpa_einsum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'njmv,kimvc->njkimc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mae_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma_einsum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0mexponential\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_einsum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mexponential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xmax/lib/python3.8/site-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_operands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/42212810/tqdm-in-jupyter-notebook-prints-new-progress-bars-repeatedly\n",
    "from pytorch.pretraining_v2 import Pretrainer\n",
    "from pytorch.models.og.model import fermiNet\n",
    "from pytorch.systems import Molecule\n",
    "\n",
    "tc.cuda.memory_summary()\n",
    "n_iterations = 10000\n",
    "n_samples = 1024\n",
    "n_electrons = 4\n",
    "n_up = 2\n",
    "n_atoms = 1\n",
    "r_atoms = tc.zeros((1, 1, 3))\n",
    "z_atoms = tc.ones((1,)) * n_electrons\n",
    "\n",
    "\n",
    "mol = Molecule(r_atoms, z_atoms, n_electrons, n_up, device='cuda')\n",
    "walkers = mol.initialise_walkers(n_walkers=1024)\n",
    "\n",
    "wf = fermiNet(mol, n_det=1, n_sh=64, n_ph=16, diagonal=False)\n",
    "\n",
    "pretrainer = Pretrainer(mol)\n",
    "pretrainer.pretrain(wf, walkers, n_it=10000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-harvey",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tc\n",
    "from pytorch.models.og.model import fermiNet\n",
    "from pytorch.pretraining_v2 import Pretrainer\n",
    "from pytorch.systems import Molecule\n",
    "\n",
    "n_el = 4\n",
    "r_atoms = tc.zeros((1, 1, 3))\n",
    "z_atoms = tc.ones((1,)) * n_el\n",
    "\n",
    "mol = Molecule(r_atoms, z_atoms, n_el, device='cuda')\n",
    "wf = fermiNet(mol, n_det=4, n_sh=64, n_ph=16, diagonal=False)\n",
    "pretrainer = Pretrainer(mol)\n",
    "pretrainer.pretrain(wf, n_it=100000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lightweight-freedom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + 2 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-advisory",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "wf = fermiNet(mol, n_det=1)\n",
    "logdet = tc.max(walkers.sum(1), dim=1, keepdims=True)[0]\n",
    "\n",
    "def l1(x, y):\n",
    "    diff = (x - y).abs().sum()\n",
    "    print(diff)\n",
    "    return diff\n",
    "\n",
    "def confirm_antisymmetric(wf, n_walkers=100):\n",
    "    up_idxs = range(0, wf.n_up)\n",
    "    down_idxs = range(wf.n_up, wf.n_el)\n",
    "    \n",
    "    walkers = tc.normal(0., 1., (5, wf.n_el, 3))\n",
    "    l1(walkers, walkers)\n",
    "    logpsi = wf(walkers)\n",
    "    print(logpsi)\n",
    "    \n",
    "    # swap two spin up\n",
    "    idxs = np.array(range(0, wf.n_el))\n",
    "    (i, j) = random.sample(up_idxs, 2)\n",
    "    idxs[i], idxs[j] = j, i\n",
    "    w_up = walkers[:, idxs, :]\n",
    "    o_up, o_down = wf.generate_orbitals(walkers)\n",
    "    print(o_up[0], '\\n', o_down[0])\n",
    "    o_up, o_down = wf.generate_orbitals(w_up)\n",
    "    print(o_up[0], '\\n', o_down[0])\n",
    "    l1(w_up, walkers)\n",
    "    logpsi_up = wf(w_up)\n",
    "    print(idxs)\n",
    "    print(tc.abs(logpsi + logpsi_up).sum())\n",
    "    print(logpsi_up)\n",
    "    \n",
    "    idxs = np.array(range(0, wf.n_el))\n",
    "    (i, j) = random.sample(down_idxs, 2)\n",
    "    idxs[i], idxs[j] = j, i\n",
    "    w_down = walkers[:, idxs, :]\n",
    "    logpsi_down = wf(w_down)\n",
    "    print(idxs)\n",
    "    print(tc.abs(logpsi + logpsi_down).sum())\n",
    "    print(logpsi_down)\n",
    "    \n",
    "    \n",
    "# confirm_antisymmetric(wf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
